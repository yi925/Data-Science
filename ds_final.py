# -*- coding: utf-8 -*-
"""DS_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PCUYU6Fi2ZUeJz5nwOz2aX2MYFCSdaQK
"""

!pip install datasets
!pip install transformers[torch]
!pip install huggingface_hub
!pip install evaluate

"""## Load Data & Data Preprocessing"""

import pandas as pd

url = 'https://docs.google.com/spreadsheets/d/1ltELQsU3VnDrwPx0YOTaeeU_PUho3ElaxanDktMn3X8/export?format=csv'

df = pd.read_csv(url)
df

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import nltk
from collections import Counter
import math
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import string

# Download NLTK Resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt_tab')

texts = df['Text']

def text_normalisation(text):

    # Initialize stemmer and lemmatizer
    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()

    # step 1: turns text into lowercase
    text_lower = text.lower()

    # Step 2: Remove punctuation
    text_no_punctuation = text_lower.translate(str.maketrans('', '', string.punctuation))

    # Step 3: Tokenize the text
    words = word_tokenize(text_lower)

    # Step 4: Remove stopwords
    stop_words = set(stopwords.words('english'))
    words_no_stopwords = [word for word in words if word not in stop_words]

    # Step 5: Normalise words with stemming and lemmatization
    normalized_words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words]

    # Join the words back into a single string
    normalized_text = ' '.join(normalized_words)

    return text

processed_text = texts.apply(text_normalisation)

def tfidf(corpus):
    vectorizer = CountVectorizer()
    term_freq_matrix = vectorizer.fit_transform(corpus).toarray()
    feature_names = vectorizer.get_feature_names_out()

    # Document count
    doc_count = len(corpus)
    df = (term_freq_matrix > 0).sum(axis=0)

    # IDF Calculation
    idf = [math.log10(doc_count / freq) if freq > 0 else 0 for freq in df]

    # TF-IDF Matrix
    tfidf_matrix = term_freq_matrix * idf

    # Create DataFrame
    return pd.DataFrame(tfidf_matrix, columns=feature_names, index=[f"Doc{i+1}" for i in range(doc_count)])

from sklearn.model_selection import train_test_split


finetune_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

finetune_df = finetune_df[['Text', 'Label']]
test_df = test_df[['Text', 'Label']]

from datasets import Dataset, DatasetDict

dataset_dict = {'finetune': Dataset.from_pandas(finetune_df),
                'test' : Dataset.from_pandas(test_df)}

dataset = DatasetDict(dataset_dict)
dataset

corpus = [finetune_df['Text'].str.cat(sep=' '),
          test_df['Text'].str.cat(sep=' ')]

tfidf_df = tfidf(corpus)
tfidf_df

import matplotlib.pyplot as plt

def visualise_tfidf(tfidf_df, doc_label, title):

    top_terms = tfidf_df.loc[doc_label].sort_values(ascending=False).head(15)

    plt.figure(figsize=(7, 5))
    plt.bar(top_terms.index, top_terms.values, color='skyblue')
    plt.title(title, fontsize=14)
    plt.ylabel("TF-IDF Score", fontsize=12)
    plt.xticks(rotation=45, fontsize=10)
    plt.tight_layout()

    plt.show()

print("TF-IDF Results:")
visualise_tfidf(tfidf_df, "Doc1", "Finetuning")
visualise_tfidf(tfidf_df, "Doc2", "Testing")

"""## black box"""

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from torch.utils.data import Dataset, DataLoader
import torch

url = "https://docs.google.com/spreadsheets/d/1ltELQsU3VnDrwPx0YOTaeeU_PUho3ElaxanDktMn3X8/export?format=csv"
df = pd.read_csv(url)


if 'Text' not in df.columns or 'Label' not in df.columns:
    raise ValueError("The dataset must contain 'Text' and 'Label' columns.")


finetune_df, test_df = train_test_split(df, test_size=0.2, random_state=42)


finetune_df = finetune_df[['Text', 'Label']]
test_df = test_df[['Text', 'Label']]


label_encoder = LabelEncoder()
finetune_df['Label'] = label_encoder.fit_transform(finetune_df['Label'])
test_df['Label'] = label_encoder.transform(test_df['Label'])


train_texts = finetune_df['Text'].tolist()
train_labels = finetune_df['Label'].tolist()
val_texts = test_df['Text'].tolist()
val_labels = test_df['Label'].tolist()


model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))


train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors="pt")
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors="pt")


class CustomDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item


train_dataset = CustomDataset(train_encodings, train_labels)
val_dataset = CustomDataset(val_encodings, val_labels)


print(f"Train dataset size: {len(train_dataset)}")
print(f"Validation dataset size: {len(val_dataset)}")


print(f"Example tokenized input: {train_dataset[0]}")

from transformers import TrainingArguments, Trainer


training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=200,
    weight_decay=0.01,
    logging_steps=50,
    save_total_limit=3,
    report_to=["tensorboard"],
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import time

# 預測並記錄推理時間
start_time = time.time()
predictions = trainer.predict(val_dataset)  # 確保 trainer 和 val_dataset 已正確定義
end_time = time.time()

# 獲取預測結果的類別
predicted_labels = predictions.predictions.argmax(-1)

# 確保預測的結果數量與標籤數量一致
assert len(predicted_labels) == len(val_labels), "Mismatch between predicted and true labels!"

# 計算評估指標
accuracy = accuracy_score(val_labels, predicted_labels)
precision = precision_score(val_labels, predicted_labels, average='weighted')
recall = recall_score(val_labels, predicted_labels, average='weighted')
f1 = f1_score(val_labels, predicted_labels, average='weighted')

# 打印模型性能指標
print(f"BERT Model Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Prediction time: {end_time - start_time:.2f} seconds")

# 打印詳細分類報告
print("\nDetailed Classification Report:")
print(classification_report(val_labels, predicted_labels, target_names=label_encoder.classes_))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt


if df['Text'].isnull().any():
    raise ValueError("The 'Text' column contains null values.")


vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
tfidf_features = vectorizer.fit_transform(df['Text']).toarray()


kmeans = KMeans(n_clusters=len(df['Label'].unique()), random_state=42)
kmeans_labels = kmeans.fit_predict(tfidf_features)


ari = adjusted_rand_score(df['Label'], kmeans_labels)
silhouette_avg = silhouette_score(tfidf_features, kmeans_labels)

print(f"KMeans Clustering Performance:")
print(f"Adjusted Rand Index (ARI): {ari}")
print(f"Silhouette Score: {silhouette_avg}")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# TF-IDF 特徵提取
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
train_tfidf = vectorizer.fit_transform(train_texts).toarray()
val_tfidf = vectorizer.transform(val_texts).toarray()

# Logistic Regression 模型訓練
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(train_tfidf, train_labels)
lr_predictions = lr.predict(val_tfidf)

# 評估 Logistic Regression 性能
lr_accuracy = accuracy_score(val_labels, lr_predictions)
lr_precision = precision_score(val_labels, lr_predictions, average='weighted')
lr_recall = recall_score(val_labels, lr_predictions, average='weighted')
lr_f1 = f1_score(val_labels, lr_predictions, average='weighted')

# 打印模型性能指標
print(f"Logistic Regression Performance:")
print(f"Accuracy: {lr_accuracy:.4f}")
print(f"Precision: {lr_precision:.4f}")
print(f"Recall: {lr_recall:.4f}")
print(f"F1 Score: {lr_f1:.4f}")

# 顯示混淆矩陣
ConfusionMatrixDisplay.from_predictions(val_labels, lr_predictions, display_labels=label_encoder.classes_)
plt.title("Confusion Matrix")
plt.show()

"""## Visualization"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
from wordcloud import WordCloud
import numpy as np
import pandas as pd
from collections import Counter
from wordcloud import STOPWORDS

"""### wordcloud"""

def plot_wordcloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('WordCloud')
    plt.show()

plot_wordcloud(' '.join(df['Text']))

"""### label distribution"""

# 0:'Democratic' 1:'Neutral' 2:'Republican'

plt.figure(figsize=(10, 6))
sns.countplot(x='Label', data=df, palette='muted')
plt.title('Distribution of Labels in Dataset')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

"""### model performance"""

bert_results = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}
kmeans_results = {'Accuracy': ari, 'Precision': 0, 'Recall': 0, 'F1 Score': 0}  # only ARI for KMeans
lr_results = {'Accuracy': lr_accuracy, 'Precision': lr_precision, 'Recall': lr_recall, 'F1 Score': lr_f1}


results_df = pd.DataFrame([bert_results, kmeans_results, lr_results],
                          index=['BERT', 'KMeans', 'Logistic Regression'])

plt.figure(figsize=(10, 6))
sns.barplot(x=results_df.index, y=results_df['Accuracy'], palette='viridis')
plt.title('Accuracy Comparison')
plt.xlabel('(ARI)')
plt.ylabel('Accuracy')
plt.show()

plt.figure(figsize=(10, 6))
results_df.plot(kind='bar', figsize=(10, 6))
plt.title('Metircs Comparison')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.show()

"""### Confusion Matrix"""

print(f"Accuracy Check: {np.sum(predicted_labels == val_labels) / len(val_labels):.4f}")

import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# 確保驗證標籤（y_true）和預測結果一致
y_true_bert = np.array(val_labels)  # 來自全局統一的 val_labels
y_pred_bert = predictions.predictions.argmax(axis=1)  # 預測結果

# 確保長度一致
assert len(y_true_bert) == len(y_pred_bert), "Mismatch between true labels and predicted labels!"

# 計算混淆矩陣
cm_bert = confusion_matrix(y_true_bert, y_pred_bert)

# 顯示混淆矩陣
disp = ConfusionMatrixDisplay(confusion_matrix=cm_bert, display_labels=label_encoder.classes_)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix for BERT")
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# 確保驗證標籤和預測結果一致
assert len(val_labels) == len(lr_predictions), "Mismatch between true labels and predicted labels for Logistic Regression!"

# 計算混淆矩陣
cm_lr = confusion_matrix(val_labels, lr_predictions)

# 顯示混淆矩陣，包含標籤名稱
disp = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=label_encoder.classes_)
disp.plot(cmap='Greens')
plt.title("Confusion Matrix for Logistic Regression")
plt.show()

"""### KMeans clusters result"""

def plot_kmeans_clusters(features, labels, kmeans_labels):
    plt.figure(figsize=(8, 6))
    plt.scatter(features[:, 0], features[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.5)
    plt.title('KMeans clusters result')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.colorbar()
    plt.show()

plot_kmeans_clusters(tfidf_features, df['Label'], kmeans_labels)

pca = PCA(n_components=2)
reduced_features = pca.fit_transform(tfidf_features)

plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)
plt.title("KMeans Clustering Results")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label='Cluster')
plt.show()

"""### Misclassified Texts analysis"""

bert_predictions = predictions.predictions.argmax(axis=1)
lr_predictions = lr.predict(val_tfidf)

bert_misclassified_idx = np.where(bert_predictions != val_labels)[0]
lr_misclassified_idx = np.where(lr_predictions != val_labels)[0]

bert_misclassified_texts = [df.iloc[i]['Text'] for i in bert_misclassified_idx]
lr_misclassified_texts = [test_df.iloc[i]['Text'] for i in lr_misclassified_idx]

bert_misclassified_labels = [val_labels[i] for i in bert_misclassified_idx]
lr_misclassified_labels = [val_labels[i] for i in lr_misclassified_idx]

bert_predicted_labels = [bert_predictions[i] for i in bert_misclassified_idx]
lr_predicted_labels = [lr_predictions[i] for i in lr_misclassified_idx]


bert_misclassified_df = pd.DataFrame({
    'Text': bert_misclassified_texts,
    'Origin Label': bert_misclassified_labels,
    'Misclassified Label': bert_predicted_labels
})

lr_misclassified_df = pd.DataFrame({
    'Text': lr_misclassified_texts,
    'Origin Label': lr_misclassified_labels,
    'Misclassified Label': lr_predicted_labels
})

# 0:'Democratic' 1:'Neutral' 2:'Republican'

bert_misclassified_df

lr_misclassified_df

import pandas as pd

lr_misclassified_df.to_csv("lr.csv", index=False)
bert_misclassified_df.to_csv("bert.csv", index=False)

from google.colab import files

files.download("lr.csv")
files.download("bert.csv")

stop_words = set(STOPWORDS)

bert_filtered_words = [word for word in " ".join(bert_misclassified_texts).split() if word.lower() not in stop_words]
lr_filtered_words = [word for word in " ".join(lr_misclassified_texts).split() if word.lower() not in stop_words]

bert_words_filtered = Counter(bert_filtered_words).most_common(20)
lr_words_filtered = Counter(lr_filtered_words).most_common(20)

bert_common_filtered_words, bert_filtered_counts = zip(*bert_words_filtered)
lr_common_filtered_words, lr_filtered_counts = zip(*lr_words_filtered)


plt.figure(figsize=(12, 6))
plt.bar(bert_common_filtered_words, bert_filtered_counts, color='blue', alpha=0.7, label='BERT')
plt.bar(lr_common_filtered_words, lr_filtered_counts, color='green', alpha=0.7, label='Logistic Regression')
plt.title('Most Common Words (Filtered) in Misclassified Texts')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend()
plt.show()

def filter_stopwords(text_list):
    stop_words = set(STOPWORDS)
    filtered_text = " ".join([word for word in text_list.split() if word.lower() not in stop_words])
    return filtered_text

filtered_bert_texts = filter_stopwords(" ".join(bert_misclassified_texts))
filtered_lr_texts = filter_stopwords(" ".join(lr_misclassified_texts))


plt.figure(figsize=(12, 6))
wordcloud_bert = WordCloud(width=800, height=400, background_color='white').generate(filtered_bert_texts)
plt.subplot(1, 2, 1)
plt.imshow(wordcloud_bert, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud - BERT Misclassified Texts')

wordcloud_lr = WordCloud(width=800, height=400, background_color='white').generate(filtered_lr_texts)
plt.subplot(1, 2, 2)
plt.imshow(wordcloud_lr, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud - Logistic Regression Misclassified Texts')

plt.tight_layout()
plt.show()

bert_text_lengths = [len(text.split()) for text in bert_misclassified_texts]
lr_text_lengths = [len(text.split()) for text in lr_misclassified_texts]

plt.figure(figsize=(10, 6))
sns.histplot(bert_text_lengths, color='blue', kde=True, label='BERT')
sns.histplot(lr_text_lengths, color='green', kde=True, label='Logistic Regression')
plt.legend()
plt.title('Distribution of Misclassified Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Count')
plt.show()

"""**Model Performance Analysis:**

In this classification task, BERT demonstrated the best overall performance, achieving the highest accuracy and F1 Score among the models evaluated. BERT’s ability to capture deep semantic relationships within the text allowed it to outperform the other methods. Logistic Regression also performed effectively, showing consistent accuracy and precision, though slightly below BERT. As expected, KMeans clustering delivered the weakest results. This can be attributed to the unsupervised nature of the algorithm, which lacks the contextual understanding necessary for effective classification in complex textual data.

When analyzing misclassification, BERT exhibited fewer errors compared to Logistic Regression, reinforcing its superior ability to handle text data. Specifically, the number of **misclassified texts by BERT totaled 9**, while **Logistic Regression misclassified 29** instances. KMeans results were not directly comparable as clustering metrics differ from supervised classification metrics.

Upon closer examination, shorter texts were disproportionately prone to misclassification by both BERT and Logistic Regression. This is likely due to the lack of sufficient contextual information within brief sentences, making it difficult for the models to establish accurate predictions. Ambiguous language and the absence of distinct political keywords further contributed to misclassification errors. For instance, tweets or texts containing sarcasm or implicit references were often challenging for the models to categorize correctly.

Additionally, certain words appeared frequently in misclassified texts. After filtering out stopwords, it was evident that these recurring terms might have influenced model sensitivity, leading to errors in classification. The visualization highlights that Logistic Regression, in particular, misclassified texts containing common yet contextually neutral words, suggesting a potential over-reliance on surface-level features.

In summary, while BERT provides robust performance in text classification, improvements can still be made in handling short, ambiguous texts. Logistic Regression, though less sophisticated, remains a viable option for simpler classification tasks. On the other hand, KMeans may require extensive feature engineering or hybrid approaches to yield meaningful results in similar contexts.
"""

